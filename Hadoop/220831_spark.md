# Spark
## 스파크의 장점
- 하둡과 유사하지만, 대량의 데이터를 메모리에 유지하는 독창적인 설계로 약 10~100배 더 빠른 속도로 같은 작업을 처리
- 현재 활발하게 개발되고 있는 병렬 처리 오픈소스엔진
- 네가지 언어를 지원 (파이썬, 자바, 스칼라, R) 
- 도커에도 스파크를 띄우는게 가능하다.
- 하둡위에 스파크를 올리는 것도 가능하다.
## 스파크에 필요한 개념
- OLTP : 빈번한 데이터 입력, 수정, 삭제 과정에서 효율성, 즉 효과적인 갱신이 주요 목표
- OLAP: 데이터를 기초로 하여 효과적으로 분석하고 조회하는 것이 목적
- 항상 대용량 데이터를 처리를 할 때 효과적이라는 것을 유념해야한다.
### 등장 배경
- 컴퓨터의 성능을 2005년 경에서 멈췄습니다. 온도때문에 cpu클럭을 더 높일 수 없는 문제 발생 -> 이후 cpu제조사들은 코어를 추가하기 시작
- 이러한 현상은 애플리케이션의 성능 향상을 위해 병렬처리가 필요
- 데이터 생성하는 디바이스 및 저장 장치는 시간이 흐를수록 낮은 가격에 형성->데이터 홍수가 일어나기 시작함
- 데이터 수집비용은 극히 저렴해졌지만, 데이터는 클러스트에서 실행을 해야했음.
- 그래서 이때 등장한 개념이 spark이다.
### 스파크의 기본 아키텍쳐
![image](https://user-images.githubusercontent.com/85923524/187357176-6d80f30d-5097-4eff-b116-2305b2bbb426.png)

## 스파크 설치 및 실행하기
### 스파크 설치
- wget https://dlcdn.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz
- tar xvfz spark-3.3.0-bin-hadoop3.tgz (압축해제)
- mv ./spark-3.3.0-bin-hadoop3 ./spark(파일명 변경)
### spark에 필요한 설정파일을 파일질라로 spark설정 파일에 넣기
- 경로는 home/hadoop/spark/conf
![image](https://user-images.githubusercontent.com/85923524/187358365-830686b2-a178-4463-8ae0-3ab78f4f0514.png)

- vi ~/.bashrc
```
export SPARK_HOME=/home/hadoop/spark
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin:$HIVE_HOME/bin:$SQOOP_HOME/bin:$SPARK_HOME/bin
```
- source ~/.bashrc
- pyspark 실행
###  spark를 실행하기 위한 jupyternotebook 환경 구축
- 실행 환경을 jupyter에서 실행하기 위한 명령어
- sudo pip3 install jupyter py4j
![image](https://user-images.githubusercontent.com/85923524/187361935-4d29dbed-ee24-4837-af37-d0e35e24584b.png)

> 에러 발생   
> spark와 jupyter 에서 python 버젼이 맞지 않아서 설치를 못한다.   
#### python 컴파일러를 직접 설치
[참고 사이트 ](https://tecadmin.net/install-python-3-7-on-centos-8/)

- 중간에 make 명령어 전에 **sudo dnf -y install  make** 이걸 먼저 실행해야함
-  find / -name "python3.7" 2>/dev/null (내가 찾고 싶은 파일을 찾는 명령어)
	- /usr/local/bin/python3.7
- sudo rm /usr/bin/python3 (기존의 python3삭제)
- sudo ln -s /usr/local/bin/python3.7 /usr/bin/python3 (기존의  python3에 링크를 삭제하고,  python3명령어가 바라보는 곳을 python 3.7.9 로 바라보게 바꾸기)
#### 바꾼 python 명령어를 python3  에서 python으로 바꾸기
- sudo ln -s /usr/local/bin/python3.7 /usr/bin/python

#### pip 버젼도 기존에서 pip3.7 버젼으로 바꾸기
- sudo ln -s /usr/local/bin/pip3.7 /usr/bin/pip

#### 이것들을 활용해서 jupyter 설치
- pip install  jupyter py4j

#### jupyter notebook  시작
- jupyter notebook --generate-config
>  오류 발생 -> 방금 설치한  pip3.7버젼에는 sqlite3가 없다.   
>  먼저 sudo yum update를 실행   
>  sudo yum install sqlite-devel.x86_64   
> cd /opt/Python-3.7.9   
> ./configure(root권한에서 실행)   
> sudo make   
> sudo make altinstall
-  jupyter notebook 실행
- 8888포트 열고 실행하기
	- jupyter notebook --ip=0.0.0.0
	-  ip 주소 (ec2 hadoop-client 주소):8888 이다.
![image](https://user-images.githubusercontent.com/85923524/187377595-e303ae37-1d70-429e-bd89-1bcf6b1587e0.png)
- 노트북 패스워드 설정
![image](https://user-images.githubusercontent.com/85923524/187567186-e96f9e86-1026-4cd7-8c29-84f8b6ef5175.png)
- 암호화된 password 복사
- vim /home/hadoop/.jupyter/jupyter_notebook_config.py
- 안에 문서에 필요한 부분을 주석 해제 후 수정
```
#134line
c.NotebookApp.allow_origin = '*'
#456line
c.NotebookApp.open_browser = False
#465line
c.NotebookApp.password = 위에서 복사한 암호
#448 line 파일을 만들어 줘야한다. mkdir ~/workspace
c.NotebookApp.notebook_dir = '/home/hadoop/workspace'

```
### miniconda로 jupyternotebook 실행
- su hadoop
- cd ~
- wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
- sh ./Miniconda3-latest-Linux-x86_64.sh
#### sh, tar, zip은 프로그램이름이다.

### spark 시작하기
- vim ~/.bashrc 에서 설정
```
export PYSPARK_DRIVER_PYTHON=jupyter
export PYSPARK_DRIVER_PYTHON_OPTS='notebook --ip=0.0.0.0'

```
- source ~/.bashrc 
-  먼저 하둡을 시작  start-dfs && start-yarn && start-mr
- pyspark --master yarn --num-executors 3
- ip주소에 :8888 쳐서 들어가기
## 스파크 살펴보기
- jupyter notebook 환경에서 spark 용어 치기(spark session 시작)
### 익스큐터
- 드라이버 프로세스가 할당한 작업을 수행
- 드라이버가 할당한 코드를 실행하고 진행 상황을 다시 드라이버 노드에 보고 하는 두 가지 역할을 수행
### 파티션
- 모든 익스큐터가 병렬로 작업을 수행할 수 있도록 파티션이라는 불리는청크 단위로 데이터를 분할 
- 만약 파티션이 하나라면 스파크에 수천 개의 익스큐처가 있더라고 병렬성은 1
### 트랜스포메이션
- 스파크의 핵심 데이터 구조는 불변성을 가진다.
- RDD- 불변성을 의미함
### 지연 연산
- 지연연산(게으른 연산)은 스파크가 연산 그래프를 처리하기 직전까지 기다리는 동작 방식을 의미
- 스파크는 특정 연산 명령어 내려진 즉시 데이터를 수정하지 않고 원시 데이터에 적용할 트랜스포메이션의 실행 계획을 생성
- 스파크는 코드를 실행하는 마지막 순간까지 대기하다가 원형 DataFrame 트랜스포메이션을 간결한 물리적 실행 계획으로 컴파일
- 이 과정을 거치며 전체 데이터 흐름을 최적화하는 엄청난 강점을 가지고 있음.
### 액션
- 사용자는 트랜스포메이션을 사용해 논리적 실행계획을 세울 수 있습니다
- 실제로 연산을 수행하려면 액션을 실행해줘야한다.
## 예제 실습
### hadoop에 폴더 생성 후 데이터 집어 넣기
#### 데이터 filezilla 혹은 note book으로 업로드
![image](https://user-images.githubusercontent.com/85923524/187580309-73870e17-c6eb-417e-8e1d-1a99043d085b.png)
#### hadoop 폴더생성
- hdfs dfs -mkdir -p /data/flight-data/csv
#### 파일옮기기 (workspace 에서)
-  hdfs dfs -put ./*.csv /data/flight-data/csv
#### 확인하기
- hdfs dfs -ls /data/flight-data/csv/
- 결과화면 
![image](https://user-images.githubusercontent.com/85923524/187580162-b1457b65-4fab-46f6-9eff-3b66896fa678.png)
### jupyter 환경에서 hadoop에 있는 데이터 읽어와서 계획 세우기
```
# hdfs 의 data/flight-data/csv/ 
#계획만 잡은 거임, 데이터 프레임을 만듬.
flightData2015 = spark\
  .read\
  .option("inferSchema", "true")\
  .option("header", "true")\
  .csv("/data/flight-data/csv/*.csv")
```
![image](https://user-images.githubusercontent.com/85923524/187581409-97d4bb7e-22ee-4267-ac79-42deeb75773c.png)
### spark의 데이터 프레임의 view로 조회, sql로 데이터 조회
```
#view 만들기(DB에서의 view)
flightData2015.createOrReplaceTempView("flight_data_2015")
#sql로 조회
sqlWay = spark.sql("""
SELECT DEST_COUNTRY_NAME, count(1)
FROM flight_data_2015
GROUP BY DEST_COUNTRY_NAME
""")
sqlWay.show(5)
#데이터프레임으로 조회
dataFrameWay = flightData2015\
  .groupBy("DEST_COUNTRY_NAME")\
  .count()

dataFrameWay.show(5)
```
- 결과 화면
![image](https://user-images.githubusercontent.com/85923524/187582064-8a4629e0-58a8-49a7-bd06-cd2693cc6f7f.png)

### 집계함수로 데이터 조회하기
```
#sql 방법
from pyspark.sql.functions import max

flightData2015.select(max("count")).take(1)


maxSql = spark.sql("""
    SELECT DEST_COUNTRY_NAME, sum(count) as destination_total
    FROM flight_data_2015
    GROUP BY DEST_COUNTRY_NAME
    ORDER BY sum(count) DESC
    LIMIT 5
""")

# dataFrame 방법
from pyspark.sql.functions import desc

flightData2015\
  .groupBy("DEST_COUNTRY_NAME")\
  .sum("count")\
  .withColumnRenamed("sum(count)", "destination_total")\
  .sort(desc("destination_total"))\
  .limit(5)\
  .show()

```
-결과 화면

![image](https://user-images.githubusercontent.com/85923524/187582447-6115ca81-94f7-4b50-a18f-a70eba2aadcc.png)
## subway DB를 활용해서 spark에서 데이터 조회하기
### 연결하기
- 터미널 창에서 cp ~/hive/lib/mysql-connector-java-8.0.21.jar ~/spark/jars/ 실행 (spark에 jar파일이 없어서 jdbc가 없다면 db에 접근이 불가능하므로 저번 실습에서 hive 폴더에 있는 jdbc관련된 jar파일에 파일을 spark에 jar폴더에 카피)
- 쥬피터 노트북에서 커널 restart후에 코드 진행
```
subway = spark.read.format("jdbc")\
      .option("url","jdbc:mysql://namenode:3306/encore")\
      .option("driver","com.mysql.cj.jdbc.Driver")\
      .option("dbtable","subway_2016")\
     .option("user","root").option("password","jun123").load()

```
- 결과화면 
![image](https://user-images.githubusercontent.com/85923524/187586757-e90706c7-6fbe-4549-ad59-7f154e21a630.png)
### 데이터 조회
- 역명 컬럼에서 중복을 제거하고 총 역의 개수 확인하기
	- spark.sql 방법으로 조회
	```
	from pyspark.sql.functions import countDistinct
	subway.select(countDistinct("역명")).show()
	```
	- view 방법으로 확인
```
	# view만들고
	subway.createOrReplaceTempView("subway")
	rt = spark.sql("""
    SELECT COUNT(DISTINCT `역명`) as st_count FROM subway;
""")
	rt.show()
```
- 결과 화면
![image](https://user-images.githubusercontent.com/85923524/187588991-6f2386d9-b850-46c4-a13d-d3ef27e958b0.png)
## 스파크의 기능 살펴보기(구조적 스트리밍)
### 주유소 데이터 폴더 올리기
- jupyter 환경에서 zip파일 업로드 
- 터미널에서 unzip retail_data.zip
- hdfs dfs -mkdir -p /data/retail-data/by-day
- hdfs dfs -put ./*.csv /data/retail-data/by-day(파일이 있는곳 /workspace/data/retail_data/by-day)
### jupyter 환경에서 데이터 불러와서 데이터 핸들링
```
# 주유소 데이터 가져오기

staticDataFrame = spark.read.format("csv")\
  .option("header", "true")\
  .option("inferSchema", "true")\
  .load("/data/retail-data/by-day/*.csv")

staticDataFrame.createOrReplaceTempView("retail_data")
staticSchema = staticDataFrame.schema

staticSchema
#정적
from pyspark.sql.functions import window, column, desc, col
rt = staticDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost").sort(desc('sum(total_cost)'))

rt.show()
```
- 결과화면
![image](https://user-images.githubusercontent.com/85923524/187605314-0aa48249-f4e7-40e1-893d-2d1188eec746.png)
#### 스트리밍코드
```
# 스트리밍 코드
streamingDataFrame = spark.readStream\
    .schema(staticSchema)\
    .option("maxFilesPerTrigger", 1)\
    .format("csv")\
    .option("header", "true")\
    .load("/data/retail-data/by-day/*.csv")

purchaseByCustomerPerHour = streamingDataFrame\
  .selectExpr(
    "CustomerId",
    "(UnitPrice * Quantity) as total_cost",
    "InvoiceDate")\
  .groupBy(
    col("CustomerId"), window(col("InvoiceDate"), "1 day"))\
  .sum("total_cost")

purchaseByCustomerPerHour.explain() #실행계획(느린연산)
```
- 결과화면
![image](https://user-images.githubusercontent.com/85923524/187605572-66331bed-c803-48f6-94c4-b27ec1640afb.png)
## 하둡이 설치를 안하고 spark 띠우기

- docker run -d -p 8888:8888 -e JUPYTER_ENABLE_LAB=yes -v /home/ec2-user/workspace:/home/joyvan --name jupyter --restart always jupyter/all-spark-notebook   

- docker logs jupyter   

- jupyter 웹 화면에서… token 값 입력하여 passwd 생성
