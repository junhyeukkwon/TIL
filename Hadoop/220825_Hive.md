# AWS Service 에서 많이 사용되는것

## [AWS_CLI](https://docs.aws.amazon.com/ko_kr/cli/latest/userguide/getting-started-install.html)

## S3

### hadoop 시작하기
-  start-dfs && start-yarn && start-mr
- ssh namenode, secondnode, datanode3 jps 구동확인
- hive 로 명령어 실행![image](https://user-images.githubusercontent.com/85923524/186548442-1254371b-6e5c-406a-afb5-7f2338356db2.png)
- CREATE TABLE FILES (line STRING); (file테이블을 생성)
- LOAD DATA INPATH '/naver/*' OVERWRITE INTO TABLE FILES; ->하둡에 있는 파일을 다 넣어버려 라는 명령어
- select * from files limit 3; ( 조회)
![image](https://user-images.githubusercontent.com/85923524/186548387-8a123231-0096-4f11-a8a8-d43aa19cffa1.png)
- query문 작성
```
CREATE TABLE word_counts AS
SELECT word, count(1) AS count FROM
(SELECT explode(split(line, ' ')) AS word FROM FILES) w
GROUP BY word
ORDER BY word;
```
>  files 테이블 내에 line을 띄어쓰기를 기준으로 나누고 그것을 word 컬럼으로 넣고 count는 word의 갯수 이고 word기준으로 묶고 word로 정렬한 word_counts 테이블을 만들어줘.
- 이렇게 하면 hive 내부에서 mapreduce 가 돈다.
 
- 데이터 조회: select * from word_counts order by count;
![image](https://user-images.githubusercontent.com/85923524/186549476-72ba06ab-2bd3-41d5-b694-0d62e2d4005c.png)

### 알뜰신잡 
#### cpu 정보 확인하기
- cat /proc/cpuinfo
#### 현재 메모리 정보 확인하기
-  free -h
## 네이버 주식 정보 크롤링해서 hive에 넣기
### 이전 강의에서 진행한다 multi_stock.py 실행
코드 

![image](https://user-images.githubusercontent.com/85923524/186555264-657469e2-2d39-4274-8669-78a35b919a92.png)

### 터미널에서 파이썬을 쓰기 위한 설정하기
- 파이썬에서 시스템 명령어 실행하기 
- 클라이언트에서 
	- sudo yum install python3 
	- sudo pip3 install ipython 
- ipython 실행
![image](https://user-images.githubusercontent.com/85923524/186555507-28994e30-0060-4c3c-902a-3a77fa01533b.png)
```
import subprocess
subprocess.call("ls -al /", shell=True)
```
### python를 이해하기
 def(*var):
 여기서  * 는 파라미터의 개수를 동적으로 받겠다는 것이다.   
a =  subprocess.check_out("ls -al /", shell=True)
이러면 변수에 담아지고, 형태는 byte 타입이고, 이건 항상 디코딩을 해줘야한다.   
a.decode("utf-8).find("var")
```
import argparse
# 인자값을 받을 수 있는 인스턴스 생성
parser = argparse.ArgumentParser(description='Argparse Tutorial')


parser.add_argument('--core',  type=int,   default=2)

# args 에 위의 내용 저장
args    = parser.parse_args()
print(args.core)

```
### hadoop-client 에서 stock.py 만들기
- su hadoop
- mkdir stock
- cd stock
-  필요한 패키지 다운 필요
	- sudo yum install python3
	- sudo pip3 install ipython
	- sudo pip3 install lxml
- vi stock.py
```
import requests, os
import pandas as pd
from multiprocessing import Pool
import argparse

def get_stock(id_):
    head = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36'}
    if os.path.isdir("./data") == False:
        os.mkdir("./data")
        
    get_url = "https://finance.naver.com/item/sise_day.naver?code={}&page={}"
    tmp = pd.concat([pd.read_html(requests.get(get_url.format(id_, cnt), headers=head).text)[0].dropna() \
    for cnt in range(1,11)])
    tmp['stock_id'] = id_
    tmp.to_csv("./data/{}.csv".format(id_), encoding='utf-8', index=False)
    
def get_master():
    url = "http://data.krx.co.kr/comm/bldAttendant/getJsonData.cmd"
    payload = {"bld" : "dbms/MDC/STAT/standard/MDCSTAT02001",
                "locale" : "ko_KR",
                "mktId" : "ALL",
                "csvxls_isNo" : "false",}
    pd.DataFrame(requests.post(url, data=payload).json()['OutBlock_1']).to_csv("./master.csv", encoding='utf-8', index=False)           
    return pd.DataFrame(requests.post(url, data=payload).json()['OutBlock_1'])['ISU_SRT_CD'].tolist()

if __name__== "__main__":
    parser = argparse.ArgumentParser(description='Argparse Tutorial')

    parser.add_argument('--core',  type=int,   default=2)

    # args 에 위의 내용 저장
    args    = parser.parse_args()

    # 리스트 정보 
    master = get_master()

    pool = Pool(processes=args.core)
    pool.map(get_stock, master)
```
-  python3 stock.py --core 1 &
-  ls -l | grep ^- | wc -l ( 현재 디렉토리의 파일 개수 확인) 
- 현재 파일 상태 
![image](https://user-images.githubusercontent.com/85923524/186560907-50f81cf8-fc71-4b9a-8466-9545243dfde6.png)

### 하둡으로 데이터 이전작업
#### 하둡 파일 상태 확인
- hdfs dfs -ls / 
#### 폴더 만들어 주기
- hdfs dfs -mkdir -p /home/hadoop/stock
#### 폴더 이전 해주기
- hdfs dfs -put ~/stock/data/*.csv /home/hadoop/stock/
### Hive로 넘어가서 작업하기
#### 테이블 생성
```
CREATE EXTERNAL TABLE IF NOT EXISTS stocks (
day STRING,
close INT,
before INT,
open INT,
high INT,
low INT,
trade INT,
symbol STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
LOCATION '/home/hadoop/stock'
tblproperties ("skip.header.line.count"="1");


```
> stock 테이블을 만들꺼고, 안에 컬럼설명이 있고, csv이니 콤마로 데이터를 구분을 할꺼다 그리고 첫줄은 스킵해 그리고 이 데이터들은 하둡의 /home/hadoop/stock 경로에 있다

- SELECT * FROM stocks LIMIT 5;
	- 위에 5개만 보여줘
- 만약에 잘못되면 DROP TABLE stocks;
#### 실행 창
![image](https://user-images.githubusercontent.com/85923524/186563166-c5d9c3a3-f713-49b0-bb49-624f03d067a2.png)
- 실행창 설명 처음에 실행했던 create구문에서 첫줄에 있는 컬럼명이 들어가 있기 때문에 그것을 제외하기 위해  **tblproperties("skip.header.line.count"="1");** 이것을 추가 했다.

### hive에서 데이터 조작
-SELECT symbol, max(close) as close_max FROM stocks GROUP BY symbol;
	- 각 주식 종목의 최대 종가를 조회
- select distinct(symbol) from stocks;
	- 중복값 제거
- select symbol, day, close, avg(close) over (order by day rows between 4 preceding and current row) as mv from stocks where symbol = '267320' order by day desc;
	- 종목, 날짜, 종가, 이동평균을 만들게(5일 기준으로) mv이름으로 별명을 붙일게 날짜기준 내림차순으로 종목은 267320만 조회를 할게
