# Hadoop 환경 설정

## 환경 설정 과정

- su hadoop
- cd /usr/lib/jvm/
- cd java-1.8.0-openjdk-1.8.0.342.b07-2.el8_6.x86_64 -> java 환경 변수를 설정하는 방법
- vi ~/.bashrc
- export JAVA_HOME="/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.342.b07-2.el8_6.x86_64"  작성
- source ~/.bashrc
- echo $JAVA_HOME 잘 불러오는 지 확인
- wget https://dlcdn.apache.org/hadoop/common/hadoop-3.2.4/hadoop-3.2.4.tar.gz -> 하둡을 설치하기
![image](https://user-images.githubusercontent.com/85923524/186042239-d595d2bb-9528-4cd8-8619-349f331a20fa.png)

- 압축 풀기 -> tar xvzf hadoop-3.2.4.tar.gz
- 해당 폴더 이름을 hadoop 변경 → mv hadoop-3.2.4 hadoop
-  filezilla로 파일 업로드
![image](https://user-images.githubusercontent.com/85923524/186043597-c1a1ee5b-1215-469e-b83f-08fd2c8d2c91.png)

-  이후 hadoop 계정에 파일 옮기기
- sudo su
-  mv *.xml /home/hadoop/hadoop/etc/hadoop/
- 다시 hadoop 계정으로 가서 cd ~/hadoop/etc/hadoop/
- ![image](https://user-images.githubusercontent.com/85923524/186044149-a3b18a4e-fbc4-495c-8800-f78f1d35bc81.png)

- hadoop 계정이 ec2-user 소유의 것을 읽기 밖에 못함
-  sudo chown hadoop:hadoop * -> 이걸로 해결 다 hadoop 계정 권한으로 바꿔라
- vi workers 내용 작성
```
datanode1
datanode2
datanode3
```
- sudo shutdown -h now -> 인스턴스 종료
-  이미지 생성한 것으로 인스턴스 3개 만들기
![image](https://user-images.githubusercontent.com/85923524/186046514-be200261-b6c6-4e8e-b4aa-3d0e8de55e95.png)

![image](https://user-images.githubusercontent.com/85923524/186046601-eb0d2245-78d2-41ee-9cf1-cc0518c0388e.png)
-  이름변경

![image](https://user-images.githubusercontent.com/85923524/186047051-2fab57c0-d7a7-4c58-b671-f44a6c2d2555.png)
- 프라이빗 ip 주소 설정
![image](https://user-images.githubusercontent.com/85923524/186047011-9b23a9d1-2ca9-4fc0-a91d-8ab69977b3f5.png)

- putty가서  client 인스턴스에 접속 
- hadoop 계정으로 로그인 -> su hadoop 
- sudo vi /etc/hosts
```
172.31.37.111 client
172.31.39.121 namenode
172.31.37.127 secondnode
172.31.39.121 datanode1
172.31.37.127 datanode2
172.31.44.143 datanode3
```
저장하고 나온 뒤 

- ssh namenode 
- sudo hostnamectl set-hostname namenode
- sudo vi /etc/hosts -> 동일한 텍스트 입력
- 같은 방법으로 진행 ( namenode, secondnode, datanode3)
<hr/>
- client에서 cd /home/hadoop/hadoop/etc/hadoop
- vi hdfs-site.xml -> 백업을 해놓기 위해서 value 3으로 설정

![image](https://user-images.githubusercontent.com/85923524/186053336-8a04b953-408a-4d17-9152-b59c1efde8fe.png)

- ssh namenode mkdir ~/data
- ssh secondnode mkdir ~/data
- ssh datanode3 mkdir ~/data
- find ./-name hadoop ( hadoop으로 이름으로 된 파일 찾기)
- vi ~/.bashrc ( 하둡 환경 변수 설정)
![image](https://user-images.githubusercontent.com/85923524/186054261-053085f0-8102-4f90-b898-17e02b473673.png)
### 같은 행동을 여러번 반복하기 힘들땐 
- scp를 이용하여 파일 전송
- scp ~/.bashrc {각각의 노드 이름}:/home/hadoop/
### ssh namenode 치기 힘들땐 ?
- alias namenode="ssh namenode"
<hr/>
- 네임노드에서  hadoop namenode -format 명령어 실행
![image](https://user-images.githubusercontent.com/85923524/186055478-73b8c5d3-2592-4ca9-b833-1e42ec322a22.png)

- 실행 결과 data폴더에 name1,2 폴더가 생성이 되는대 이전에 3개의 복사본으로 만든것에 대해 namenode가 장부 같은 역할을 하는 것이다.

<hr/>

서버 실행
- source ~/.bashrc
- ssh namenode start-dfs.sh
- ssh secondnode start-yarn.sh
- ssh namenode mr-jobhistory-daemon.sh start historyserver
- ssh namenode를 들어가서 jps (java프로세스 실행 목록 보기)명령어를 치면 목록이 뜬다.
- ssh secondnode 에서도, ssh datanode3에서도 jps로 확인 
![image](https://user-images.githubusercontent.com/85923524/186057105-e82cfd7a-521c-48d7-b47b-188a54a38256.png)
- 로그 보는 법 : /home/hadoop/hadoop/logs
<br/>
### client에서 hadoop 시작할 때 명령어 bashrc파일에다가 넣기
```
alias start-dfs="ssh namenode start-dfs.sh"
alias start-yarn="ssh secondnode start-yarn.sh"
alias stop-dfs="ssh namenode stop-dfs.sh"
alias stop-yarn="ssh secondnode stop-yarn.sh"
alias start-mr="ssh namenode mr-jobhistory-daemon.sh start historyserver"
alias stop-mr="ssh namenode mr-jobhistory-daemon.sh stop historyserver"
```
- 프로세스 시작 순서
	1. start-dfs
	2. start-yarn
	3. start-mr
- 프로세스 종료 순서
	1. stop-yarn
	2. stop-mr
	3. stop-dfs
<hr/>
## hadoop(webhard)에 뉴스를 크롤링해서 데이터 넣기

-하둡 시작후 폴더 만들기 hdfs dfs -mkdir /mydata
- hdfs dfs -put ~/hadoop/etc/hadoop/*.xml /mydata ->mydata 폴더 안에 hadoop/etc/hadoop에 있는 xml 파일을 다 넣는다.( 복제) 
-  맵리듀스를 하기(데이터가 흩어져있지만, 데이터를 각각의 컴퓨터에서 집계처리를 해서 중앙에서 전체적으로 집계(reduce)를 하면됨) -> 이게 상당히 느려서 spark를 사용해서 속도를 높힌다.
- hadoop jar ~/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.4.jar grep /mydata /output 'dfs[a-z]+'
- hdfs dfs -cat /output/* -> dfs의 모든 아웃풋을 알아낸다.
- namenode의 IP주소 뒤에 :50070 을 붙이고 웹창을 띄우면 hadoop관리창이 뜬다.
-  이전에 보안 그룹에서 모든 TCP와 내 ip주소를 등록해야지 웹에 접근이 가능하다.
![image](https://user-images.githubusercontent.com/85923524/186116271-e4c6b75d-57b2-4458-abd9-2aee13e44cfd.png)

![image](https://user-images.githubusercontent.com/85923524/186116806-87df33b3-039e-4e90-85f6-84b06dcd8212.png)
